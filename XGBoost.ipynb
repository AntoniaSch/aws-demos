{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direkt Marketing mit Amazon SageMaker XGBoost\n",
    "\n",
    "#### Supervised Learning: Binäre Klassifikation\n",
    "\n",
    "Letztes Update: April, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install sagemaker awscli boto3 smdebug --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# TEIL 1 - Herunterladen und Verarbeiten des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst müssen wir den [direkt marketing datensatz](https://archive.ics.uci.edu/ml/datasets/bank+marketing) von UCI's ML Repository downloaden. Anschließend werden wir die csv-Datei lesen, und für unser ML Modell vorbereiten. Dafür sollten wir erstmal verstehen, was für Daten wir zur Verfügung haben und was wir vorhersagen möchten (= Ziel Variable Y).\n",
    "\n",
    "Der Datensatz enthält Informationen über Direkt Marketing Kampagnen einer portugiesischen Bank. Diese Kampagnen basieren auf Telefonaten in denen der Kunde ein Produkt (bank term deposit) kaufen kann. Der Ausgang des Telefonates ist in der Variable Y gespeichert und wird Ziel unserer Klassifikation (yes/no) sein. \n",
    "\n",
    "Wir möchten also mit unserem ML Model vorhersagen, ob ein Kunde positiv oder negativ auf das Marketing Angebot per Telefon reagiert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile                                    \n",
    "!wget -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
    "!unzip -o bank-additional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=';')\n",
    "pd.set_option('display.max_columns', 100)     \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf den ersten Blick können wir sehen wir haben über 40k Zeilen mit verschiedenen Kundendaten und je 20 beschreibenden Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape # (Anzahl Zeilen, Anzahl Spalten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst sollten wir ein Gefühl für die Ziel-Variable y und deren Verteilung erhalten. Wir stellen fest, dass wir deutlich mehr \"no\" Beispiele haben und dementsprechend ein unausgewogenen Datensatz haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verhältnis von negativen zu positiven Werten: {}\".format( data['y'].value_counts()[0]/data['y'].value_counts()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datensatz vorbereiten für Machine Learning Modelle\n",
    "Den Datensatz zu bereinigen und zu transformieren ist ein signifikanter Teil eines jeden Machine Learning Projekts. Das Behandeln von Ausreißern, das Füllen von fehlenden Werten oder die Erstellung bzw. Transformation neuer Variablen sind nur Beispiele der möglichen Vorbereitungen. Wir tun dies mit dem Ziel wichtige Informationen herauszuarbeiten, die es dem Model einfacher machen zu lernen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst können wir uns den Datensatz mit `.describe()` genauer ansehen und nach ersten Auffälligkeiten untersuchen:\n",
    "Die Variable **pdays** (Anzahl der Tage seitdem letzten Kontakt zum Kunden) z.B. hat als Maximum 999 Tage - das könnte ein Platzhalter sein für den Fall, dass ein Kunde noch gar nicht kontaktiert worden ist. Daher macht es Sinn, eine neue Spalte **no_previous_contact** zu erstellen. Sämtliche Reihen mit `pdays==999` werden eine \"0\" erhalten, ansonsten \"1\". Anschließend können wir die Spalte **pdays** löschen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)\n",
    "data.drop(['pdays'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Spalte **job** hat viele Kategorien, die wir in aussagekräftigere Kategorien verwandeln können: \n",
    "Da es wahrscheinlich ist, dass die Berufstätigkeit des Kunden einen Einfluss auf die Zielvariable hat, können wir eine neue Variable **not_working** erstellen und dort \"student\", \"retire\" und \"unemployed\" zusammenfassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu guter letzt kodieren wir die kategorialen Variablen in eine Reihe von Dummy Variablen mit pandas `get_dummies()`Funktion. Das bedeutet, dass wir aus jeder kategorialen Ausprägung einer Variable eine neue Spalte erstellen (auch bekannt als \"One-Hot-Encoding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(data)  # Convert categorical variables to sets of indicators\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir aus jeder kategorialen Ausprägung einer Variable (wie z.B.**job**,**marital** etc.) viele neue Variablen erstellt und somit aus 21 schlussendlich 67 Variablen gemacht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unterteilen des Datensatzes\n",
    "\n",
    "Als nächstes splitten wir den Datensatz in drei seperate Datensätze: (70%), validation (20%) und test (10%). Während des Trainings werden wir versuchen die Performance beim Validation datensatz zu maximieren - sobald das Model fertig bereitgestellt ist, werden wir erneut die Performance auf dem Test datensatz evaluieren. \n",
    "\n",
    "Im Folgenden werden wir Amazon SageMaker's XGBoost algorithmus verwenden, welcher Daten entweder im libSVM oder CSV Format erwartet. Im Rahmen dieser Demo, werden wir uns auf das CSV-Format konzentrieren. Die erste Spalte der Datei mus sunsere Zielvariable y sein - ebenso sollte die datei keine Überschriften haben in den Spalten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed to 123 for reproductibility\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=123), \n",
    "                                                  [int(0.7 * len(model_data)), int(0.9*len(model_data))])  \n",
    "\n",
    "# Drop the two columns for 'yes' and 'no' and add 'yes' back as first column of the dataframe\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "\n",
    "# Dropping the target value, as we will use this CSV file for batch transform\n",
    "test_data.drop(['y_no','y_yes'], axis=1).to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes müssen wir die Dateien in Amazon S3 Bucket hochladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3, os\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()                     \n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusätzlich müssen wir SageMaker sagen, wo die Trainings und Validierungs Daten gespeichert sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n",
    "s3_data = {'train': s3_input_train, 'validation': s3_input_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Teil 2 - Model erstellen und trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie anfangs erläutert, handelt es sich bei unserem use case um ein Klassifizierungsproblem. *XGBoost* ist ein beliebtes open-source Projekt für Gradient Boosting Trees und wurde erfolgreich verwendet in vielen machine learning Wettbewerben! SageMaker bietet eine entsprechendes managed framework an, welches wir im Folgenden benutzen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name    \n",
    "\n",
    "container = sagemaker.image_uris.retrieve('xgboost', region,version='0.90-2')\n",
    "\n",
    "\n",
    "xgb = Estimator(\n",
    "    container,                                               # container des algorithmus XGBoost)\n",
    "    role=sagemaker.get_execution_role(),                     # IAM Berechtigungen für SageMaker\n",
    "    sagemaker_session=sess,                                  # Technisches Object\n",
    "                                    \n",
    "    input_mode='File',                                       # Kopieren des Datensatzes und anschließendes Trainieren \n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),  # S3 Pfad, wo das Model gespeichert wird\n",
    "                                    \n",
    "    instance_count=1,                                  # Instance Spezifikationen\n",
    "    instance_type='ml.m4.2xlarge',\n",
    "                                    \n",
    "    use_spot_instances=True,                           # Benutzung von spot instance\n",
    "    max_run=300,                                       # Maximale Trainingszeit\n",
    "    max_wait=600,                                      # Maximale Trainingszeit + Wartezeit auf spot instances.\n",
    "                                    \n",
    "    debugger_hook_config=DebuggerHookConfig(                 # Save training tensors\n",
    "        s3_output_path='s3://{}/{}/debug'.format(bucket, prefix), \n",
    "        collection_configs=[\n",
    "            CollectionConfig(\n",
    "                name=\"metrics\",\n",
    "                parameters={\n",
    "                    \"save_interval\": '1'\n",
    "                }\n",
    "            ),\n",
    "            CollectionConfig(\n",
    "                name=\"feature_importance\",\n",
    "                parameters={\n",
    "                    \"save_interval\": '1'\n",
    "                }\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    "    \n",
    "    rules=[\n",
    "        Rule.sagemaker(                                      # Konfiguration der Debugger Regel\n",
    "            rule_configs.class_imbalance(),                  \n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\"\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spezifizierung der hyperparameter\n",
    "Jeder SageMaker integrierte Algorithmus hat verschiedene Hyperparameter, die je nach use case und Dateneigenschaften gesetzt werden müssen. Für XGBoost, gibt es folgende [Hyperparameter](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html).\n",
    "\n",
    "Im Rahmen der Demo werden wir lediglich die folgenden drei Hyperparameter benutzen:\n",
    "* Binäre Klassifikation: 'binary:logistic'.\n",
    "* Zur Evaluierung der Performance verwenden wir die 'Area Under Curve' Metrik. \n",
    "* Wir möchten maximal 100 Runden trainieren- sollte die AUC Metrik sich in 10 Runden nicht verbessern, beenden wir das Training vorzeitig.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='auc', \n",
    "    num_round=100,\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun sind alle Parameter spezifiziert und wir können das Training beginnen mit der `.fit()` Funktion und den entsprechenden S3 Pfäden zu den Training/Validation CSV Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(s3_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the tensors saved during training, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smdebug\n",
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = xgb.latest_job_debugger_artifacts_path()\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our metric over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    \"\"\"\n",
    "    For the given tensor name, walks though all the iterations\n",
    "    for which you have data and fetches the values.\n",
    "    Returns the set of steps and the values.\n",
    "    \"\"\"\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = [tensor.value(s) for s in steps]\n",
    "    return steps, vals\n",
    "\n",
    "def plot_collection(trial, collection_name, regex='.*',figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Takes a `trial` and a collection name, and \n",
    "    plots all tensors that match the given regex.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.despine()\n",
    "\n",
    "    tensors = trial.collection(collection_name).tensor_names\n",
    "\n",
    "    for tensor_name in sorted(tensors):\n",
    "        if re.match(regex, tensor_name):\n",
    "            steps, data = get_data(trial, tensor_name)\n",
    "            ax.plot(steps, data, label=tensor_name)\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.set_title(collection_name)\n",
    "    ax.set_xlabel('Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collection(trial, \"metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes können wir die Feature Importance visualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(trial, importance_type=\"weight\"):\n",
    "    SUPPORTED_IMPORTANCE_TYPES = [\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"]\n",
    "    if importance_type not in SUPPORTED_IMPORTANCE_TYPES:\n",
    "        raise ValueError(f\"{importance_type} is not one of the supported importance types.\")\n",
    "    plot_collection(\n",
    "        trial,\n",
    "        \"feature_importance\",\n",
    "        regex=f\"feature_importance/{importance_type}/.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable 1 (job) und 5 (housing) sollten die wichtigsten Variablen sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Teil 3 - Bereitstellen und Einsetzen des Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante 1: Vorhersagen mit einem SageMaker Endpunkt\n",
    "\n",
    "Nun können wir unser trainiertes Model mit einem HTTPS Endpunkt! Anschließend können wir Daten zu dem Endpunkt senden und eine Vorhersage bzw Klassifkation erhalten.\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint. Our data is currently stored as NumPy arrays in memory of our notebook instance. To send it in an HTTP POST request, we'll serialize it as a CSV string and then decode the resulting CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "xgb_endpoint = xgb.deploy(\n",
    "    endpoint_name = 'DEMO-xgboost-dm',\n",
    "    initial_instance_count = 1,                    \n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    serializer = CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_endpoint.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.drop(['y_no', 'y_yes'], axis=1).to_numpy())\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele Möglichkeiten die Performance von XGBoost zu visualisieren und zu messen: Wir können z.B. mit einer Confusion Matrix erkennen, in welchem Verhältnis Vorhersagen und tatsächliche Werte (\"ground truth\") stehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Von den ~4000 potenziellen Kunden haben wir also vorhergesagt, dass 136 ein Abonnement abschließen würden, und 94 von ihnen haben es tatsächlich getan. Wir hatten auch 389 Abonnenten, die sich angemeldet haben, die wir nicht vorhergesagt haben. Das ist weniger als wünschenswert, aber das Modell kann (und sollte) abgestimmt werden, um dies zu verbessern\n",
    "\n",
    "Für jede Stichprobe liefert unser binärer Klassifikator eine Wahrscheinlichkeit zwischen 0 und 1. Da wir uns entschieden haben, die Genauigkeit zu maximieren, legt das Modell einen Schwellenwert von 0,5 fest: Alles, was darunter liegt, wird als 0 behandelt, alles, was darüber liegt, als 1.\n",
    "\n",
    "Um ein wenig tiefer einzutauchen: Der Schwellenwert ist in der Metrik enthalten, die XGBoost verwendet. Hier verwenden wir die Standard-Metrik \"eval_metric\" für die Klassifizierung, d. h. \"error\". Diese Metrik hat einen Standard-Schwellenwert von 0,5. Wenn Sie sich die XGBoost-Dokumentation ansehen (https://xgboost.readthedocs.io/en/latest/parameter.html), werden Sie sehen, dass es möglich ist, einen anderen Schwellenwert zu übergeben, indem Sie etwas wie folgt tun: xgb.set_hyperparameters(objective='binary:logistic', num_round=100, eval_metric='error@0.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variante 2: Vorhersagen mit Batch Transform Jobs\n",
    "**Zur Erinnerung: test.csv sollte lediglich die features enthalten, jedoch nicht die Zielvariable y. Ebenso sollte die Datei keine Header row enthalten**\n",
    "\n",
    "Manche Anwendungen benötigen keine HTTPS-basierende real-time Vorhersagen - z.B. einmal die Woche 10GB Daten zu verarbeiten und Vorhersagen zu treffen mit einem endpoint wäre nicht effizient.\n",
    "Für solche Fehler gibt es die Möglichkeit SageMaker Batch Transforms zu nutzen: Wir kreiren ein Transformer Objekt und senden die Daten von S3 und erhalten ebenfalls die Vorhersagen in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = xgb.transformer(instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "transformer.transform('s3://{}/{}/test/test.csv'.format(bucket, prefix), content_type='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.wait()\n",
    "print(\"Die Batch-Vorhersage wurde hier gespeichert {}\".format(transformer.output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Vorhersagen in S3\n",
    "Der Output eines Batch Transforms landet in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(transformer.output_path+'/test.csv.out',header=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Löschen des Endpunktes\n",
    "Sobald wir fertig sind mit dieser Demo sollten wir den Endpoint löschen, um unnötige Kosten zu vermeiden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_endpoint.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
